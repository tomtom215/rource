<?xml version="1.0" encoding="UTF-8"?>
<!--
  INSIGHTS ENGINE — NEXT SESSION CONTINUATION PROMPT
  Project: Rource (Rust + Gource VCS visualization)
  Standard: PEER REVIEWED PUBLISHED ACADEMIC
  Generated: 2026-02-10
  Branch: claude/enhance-visualization-insights-wzc96
  Previous Session Commit: bbd2f5b (feat(insights): add research-backed repository analytics engine)

  PURPOSE: This document is a self-contained, copy-ready prompt for the next
  Claude Code session. It contains the full context of what was implemented,
  what remains, exact file locations, standards, and constraints so that the
  next session can begin immediately at full velocity with zero ramp-up.
-->

<session-prompt>

  <!-- ================================================================== -->
  <!-- SECTION 1: MANDATORY CONTEXT AND STANDARDS                          -->
  <!-- ================================================================== -->

  <standards>
    <title>Non-Negotiable Quality Standards</title>
    <description>
      Every line of code, every metric, every claim MUST meet PEER REVIEWED
      PUBLISHED ACADEMIC standards suitable for top-tier venues (PLDI, POPL, CAV).
      There are ZERO exceptions. There are ZERO shortcuts. There is ZERO "good enough."
    </description>

    <rule id="R1">Never guess. Never assume. Never overstate. Never exaggerate.</rule>
    <rule id="R2">Never approximate measurements. "~50%" is WRONG. "52.3% (2.41us to 1.15us)" is correct.</rule>
    <rule id="R3">Never claim performance improvements without criterion benchmarks (100+ samples, 95% CI, before AND after).</rule>
    <rule id="R4">Never dismiss any issue as "pre-existing" or "unrelated to my changes." Fix it NOW.</rule>
    <rule id="R5">Never commit without: cargo test, cargo clippy --all-targets --all-features -- -D warnings, cargo fmt --check.</rule>
    <rule id="R6">Never skip documentation. Every change documented with exact metrics.</rule>
    <rule id="R7">Every nanosecond matters. Frame budget is 20 microseconds (50,000 FPS target). 1us = 5% of budget = 3,000 CPU cycles.</rule>
    <rule id="R8">If it cannot be measured, it did not happen. If it was not tested, it does not work.</rule>
    <rule id="R9">Each session MUST be remarkably better than the previous session. No violations, no exceptions, no compromises.</rule>
    <rule id="R10">Insights computation happens at load/query time, NOT per-frame. Zero impact on rendering frame budget.</rule>
  </standards>

  <!-- ================================================================== -->
  <!-- SECTION 2: WHAT WAS IMPLEMENTED (SESSION 1 — COMPLETE)              -->
  <!-- ================================================================== -->

  <completed-work>
    <title>Session 1 Implementation Summary</title>
    <commit>bbd2f5b — 8 files, 2,470 lines, 52 tests, zero warnings</commit>
    <branch>claude/enhance-visualization-insights-wzc96</branch>

    <architecture>
      <description>
        VCS-agnostic insights engine in rource-core with WASM API layer.
        Single-pass accumulation over commit history. All computation at
        query time (not per-frame). No new external dependencies added.
        Uses rustc_hash::FxHashMap for performance-critical paths.
      </description>
      <data-flow>
        Vec&lt;rource_vcs::Commit&gt; --[convert_commits]--> Vec&lt;CommitRecord&gt; --[compute_insights]--> InsightsReport --[format_*_json]--> JSON String
      </data-flow>
    </architecture>

    <modules>
      <module name="hotspot" file="crates/rource-core/src/insights/hotspot.rs" tests="7">
        <algorithm>Exponential decay-weighted change frequency</algorithm>
        <research>Nagappan et al. (2005) — relative code churn predicts fault-prone components with approximately 89% accuracy</research>
        <formula>score = weighted_changes * (1 + ln(1 + total_changes))</formula>
        <formula>weight = exp(-lambda * (t_max - t_commit)), lambda = ln(2) / (0.5 * time_span)</formula>
        <types>FileHotspot, HotspotAccumulator</types>
        <constants>HALF_LIFE_FRACTION = 0.5</constants>
      </module>

      <module name="coupling" file="crates/rource-core/src/insights/coupling.rs" tests="8">
        <algorithm>Sparse co-change matrix with support/confidence metrics</algorithm>
        <research>D'Ambros et al. (2009) — coupling correlates with defects better than OO complexity metrics</research>
        <formula>confidence_A_to_B = support / total_changes_A</formula>
        <complexity>O(sum of k_i^2) where k_i = files per commit i (bounded by BULK_COMMIT_THRESHOLD=50)</complexity>
        <types>CouplingPair, CouplingAccumulator</types>
      </module>

      <module name="ownership" file="crates/rource-core/src/insights/ownership.rs" tests="10">
        <algorithm>Greedy set cover for bus factor computation</algorithm>
        <research>Bird et al. (2011) "Don't Touch My Code!", Avelino et al. (2016) bus factor study</research>
        <description>
          Greedy set cover: repeatedly remove contributor covering most files
          until some file becomes orphaned. Returns minimum contributors needed
          to maintain knowledge of each directory.
        </description>
        <types>FileOwnership, ContributorShare, DirectoryBusFactor</types>
      </module>

      <module name="temporal" file="crates/rource-core/src/insights/temporal.rs" tests="11">
        <algorithm>Sliding-window burst detection + 7x24 activity heatmap</algorithm>
        <research>Eyolfson et al. (2011) — late-night commits more error-prone; Nagappan et al. (2008) — bursts predict defects</research>
        <types>TemporalReport, ChangeBurst, TemporalAccumulator</types>
        <constants>BURST_WINDOW_SECONDS = 3600, BURST_THRESHOLD = 10</constants>
      </module>

      <module name="orchestrator" file="crates/rource-core/src/insights/mod.rs" tests="9">
        <algorithm>Single-pass accumulation with Shannon entropy computation</algorithm>
        <research>Hassan (2009) JIT-SDP — commit entropy as defect predictor</research>
        <formula>entropy = log2(file_count) for file_count >= 2, else 0</formula>
        <types>FileActionKind, FileRecord, CommitRecord, InsightsReport, SummaryStats</types>
        <constants>BULK_COMMIT_THRESHOLD = 50, MIN_COUPLING_SUPPORT = 2, DEFAULT_TOP_N = 50</constants>
      </module>

      <module name="wasm_api" file="rource-wasm/src/wasm_api/insights.rs" tests="8">
        <endpoints>
          <endpoint name="getInsights" returns="Full JSON report" />
          <endpoint name="getHotspots" returns="Top N hotspots" args="limit: Option&lt;usize&gt;" />
          <endpoint name="getChangeCoupling" returns="Co-change pairs" args="limit: Option&lt;usize&gt;" />
          <endpoint name="getBusFactors" returns="Bus factor per directory" />
          <endpoint name="getTemporalPatterns" returns="Activity heatmap + bursts" />
          <endpoint name="getInsightsSummary" returns="Dashboard summary" />
        </endpoints>
        <note>Hand-built JSON serialization via std::fmt::Write (no serde dependency)</note>
      </module>
    </modules>

    <quality-gates>
      <gate name="tests">52 new tests (44 core + 8 WASM), all passing</gate>
      <gate name="clippy">Zero warnings with --all-targets --all-features -D warnings</gate>
      <gate name="fmt">Zero formatting diffs</gate>
      <gate name="workspace">All 3,016+ workspace tests pass with zero failures</gate>
    </quality-gates>
  </completed-work>

  <!-- ================================================================== -->
  <!-- SECTION 3: AVAILABLE DATA (What the VCS parser provides)            -->
  <!-- ================================================================== -->

  <available-data>
    <title>Data Available per Commit (rource_vcs::Commit)</title>
    <description>
      All insights MUST be computed from this data alone. No external APIs,
      no file system access, no git blame, no line counts. The VCS parser
      provides ONLY the following fields. Any insight requiring data beyond
      these fields is NOT implementable without extending the parser.
    </description>

    <field name="hash" type="String" required="true">Commit hash (Git SHA-1, SVN revision number)</field>
    <field name="timestamp" type="i64" required="true">Unix epoch seconds (supports negative for pre-1970)</field>
    <field name="author" type="String" required="true">Commit author name</field>
    <field name="email" type="Option&lt;String&gt;" required="false">Author email (may not be available for all VCS types)</field>
    <field name="files" type="Vec&lt;FileChange&gt;" required="false">List of file changes in commit</field>

    <file-change-fields>
      <field name="path" type="PathBuf">File path relative to repository root</field>
      <field name="action" type="FileAction">Create, Modify, or Delete (no Rename distinction)</field>
    </file-change-fields>

    <critical-limitations>
      <limitation id="L1">NO line count / diff size — cannot compute lines added/removed per file</limitation>
      <limitation id="L2">NO commit message — cannot mine for keywords, issue references, revert patterns</limitation>
      <limitation id="L3">NO parent commit hash — cannot build DAG or detect merge commits</limitation>
      <limitation id="L4">NO branch information — cannot distinguish main vs feature branch commits</limitation>
      <limitation id="L5">NO file size / complexity metric — must infer from change frequency only</limitation>
      <limitation id="L6">NO rename tracking as distinct action — renames appear as Modify</limitation>
      <limitation id="L7">Extending the parser to provide additional fields is an option but requires upstream changes to rource-vcs</limitation>
    </critical-limitations>
  </available-data>

  <!-- ================================================================== -->
  <!-- SECTION 4: FUTURE INSIGHTS — RANKED BY IMPLEMENTATION PRIORITY      -->
  <!-- ================================================================== -->

  <future-insights>
    <title>Research-Backed Insights for Future Implementation</title>
    <description>
      The following insights are candidates for future sessions. Each is
      categorized by implementation feasibility given the available data,
      research backing, and expected value to engineering teams.

      CRITICAL: Before implementing ANY insight, verify that the required
      data fields are available (see Section 3). If a field is missing,
      the insight either requires a parser extension or must be adapted
      to work without that field.
    </description>

    <!-- ============================================================== -->
    <!-- TIER 1: IMPLEMENTABLE NOW (data already available)              -->
    <!-- ============================================================== -->

    <tier id="1" name="Implementable with Current Data" priority="HIGH">
      <description>
        These insights can be computed from the existing CommitRecord data
        (timestamp, author, files with path and action). No parser changes needed.
      </description>

      <insight id="I1" name="Knowledge Map / Knowledge Silos" complexity="MEDIUM">
        <research>
          Rigby &amp; Bird (2013) "Convergent Contemporary Software Peer Review Practices" (ESEC/FSE 2013);
          Fritz et al. (2014) degree-of-knowledge model
        </research>
        <description>
          Extend the existing ownership module to compute per-file knowledge
          distribution maps. Identify "knowledge silos" — files where only
          one person has ever contributed. These represent high-risk areas
          where institutional knowledge is concentrated.
        </description>
        <data-required>author, files[].path — already available</data-required>
        <algorithm>
          For each file, compute a knowledge vector: author -> change_count.
          Knowledge silo = file where top_owner_share > 0.8 AND contributor_count == 1.
          Knowledge breadth = Shannon entropy of the ownership distribution per file.
          Low entropy = concentrated knowledge = higher risk.
        </algorithm>
        <output>
          Per-file: knowledge_entropy, is_silo (bool), silo_owner.
          Per-directory: avg_knowledge_entropy, silo_percentage, most_siloed_files.
          Global: total_silos, percentage_of_codebase_siloed.
        </output>
        <value>
          Engineering managers can identify files that will become unmaintainable
          if a single developer leaves. Directly actionable: assign code reviews
          to spread knowledge.
        </value>
        <wasm-api>getKnowledgeMap(), getKnowledgeSilos(limit)</wasm-api>
      </insight>

      <insight id="I2" name="Developer Collaboration Network" complexity="MEDIUM">
        <research>
          Meneely &amp; Williams (2011) "Socio-Technical Developer Networks" (ESEM 2011);
          Bird et al. (2009) "Putting It All Together: Using Socio-Technical Networks" (ICSE 2009)
        </research>
        <description>
          Build an implicit collaboration graph where two developers are connected
          if they modify the same file. Edge weight = number of shared files.
          Reveals team structure, collaboration patterns, and isolated contributors.
        </description>
        <data-required>author, files[].path — already available</data-required>
        <algorithm>
          Build file -> Set&lt;author&gt; map from commit history.
          For each file touched by multiple authors, create edges between all author pairs.
          Edge weight = number of shared files.
          Compute: degree centrality (most connected), betweenness (bridges between teams),
          connected components (isolated teams), clustering coefficient.
        </algorithm>
        <output>
          Per-author: degree, betweenness_centrality, cluster_id.
          Global: num_clusters, largest_cluster_size, isolated_developers.
          Edges: author_a, author_b, shared_file_count, files_list.
        </output>
        <value>
          Identifies isolated developers, team boundaries, and collaboration bottlenecks.
          Network visualization potential for future frontend integration.
        </value>
        <wasm-api>getCollaborationNetwork(), getIsolatedDevelopers()</wasm-api>
      </insight>

      <insight id="I3" name="Codebase Growth Trajectory" complexity="LOW">
        <research>
          Lehman (1996) Laws of Software Evolution (Lehman's Laws);
          Herraiz et al. (2007) "Towards a Simplification of the Bug Report Form" (MSR 2007) — growth analysis
        </research>
        <description>
          Track codebase size (number of active files) over time. Compute growth
          rate, acceleration, and projected file count. Detect periods of rapid
          growth vs consolidation.
        </description>
        <data-required>timestamp, files[].path, files[].action — already available</data-required>
        <algorithm>
          Maintain a running set of active files (add on Create, remove on Delete, ignore Modify).
          At each commit, record (timestamp, active_file_count).
          Compute: growth_rate (files/month), acceleration (delta of growth_rate),
          growth_periods (intervals where rate > threshold),
          consolidation_periods (intervals where deletes > creates).
          Linear regression for trend line: y = mx + b over time series.
        </algorithm>
        <output>
          Time series: [(timestamp, active_files, creates_cumulative, deletes_cumulative)].
          Global: current_file_count, total_created, total_deleted, net_growth,
          avg_monthly_growth, growth_trend (accelerating/stable/decelerating).
          Periods: [(start, end, type: growth|consolidation, rate)].
        </output>
        <value>
          Shows whether the codebase is growing sustainably or exploding.
          Useful for capacity planning and tech debt assessment.
        </value>
        <wasm-api>getCodebaseGrowth(), getGrowthTimeSeries()</wasm-api>
      </insight>

      <insight id="I4" name="Work-Type Mix Analysis" complexity="LOW">
        <research>
          Hindle et al. (2008) "Large-Scale Model of Software Development Activity" (MSR 2008);
          Mockus &amp; Votta (2000) "Identifying Reasons for Software Changes" (ASE 2000)
        </research>
        <description>
          Classify commits by the ratio of Create/Modify/Delete actions to infer
          whether the team is primarily building new features (high Create ratio),
          maintaining existing code (high Modify ratio), or cleaning up (high Delete ratio).
        </description>
        <data-required>files[].action — already available</data-required>
        <algorithm>
          For each commit, compute: create_ratio, modify_ratio, delete_ratio.
          Classify: "feature" (create_ratio > 0.5), "maintenance" (modify_ratio > 0.7),
          "cleanup" (delete_ratio > 0.3), "mixed" (otherwise).
          Aggregate over time windows (weekly/monthly) to show work-type trends.
        </algorithm>
        <output>
          Per-commit: classification, create_ratio, modify_ratio, delete_ratio.
          Per-period: feature_commits_pct, maintenance_commits_pct, cleanup_commits_pct.
          Global: dominant_work_type, work_type_trend (shifting toward maintenance?).
        </output>
        <value>
          Shows whether team effort is shifting from building to maintaining —
          a key indicator of project maturity and potential tech debt accumulation.
        </value>
        <wasm-api>getWorkTypeMix(), getWorkTypeTrend()</wasm-api>
      </insight>

      <insight id="I5" name="File Lifecycle Analysis" complexity="MEDIUM">
        <research>
          Godfrey &amp; Tu (2000) "Evolution in Open Source Software" (IWPSE 2000);
          Gall et al. (1998) "Detection of Logical Coupling Based on Product Release History" (ICSM 1998)
        </research>
        <description>
          Track the full lifecycle of each file: creation, modification history,
          and eventual deletion. Identify short-lived files (created and deleted
          within a short window), which may indicate churn or experimental code.
        </description>
        <data-required>timestamp, files[].path, files[].action — already available</data-required>
        <algorithm>
          For each file, record: first_seen (Create timestamp), last_modified,
          delete_time (if deleted), modification_count, unique_authors.
          Classify lifecycle: "stable" (few changes, long-lived), "volatile" (many changes),
          "ephemeral" (created and deleted within X days), "dead" (no changes in Y months).
          Survival analysis: Kaplan-Meier estimator for file survival probability over time.
        </algorithm>
        <output>
          Per-file: lifecycle_stage, age_days, modifications_per_month, is_ephemeral, is_dead.
          Global: avg_file_lifespan, ephemeral_file_rate, churn_rate (created+deleted / total).
          Survival curve: [(age_bucket, survival_probability)].
        </output>
        <value>
          High ephemeral rate suggests poor upfront design or excessive experimentation.
          Dead file detection helps codebase cleanup.
        </value>
        <wasm-api>getFileLifecycles(limit), getEphemeralFiles(threshold_days)</wasm-api>
      </insight>

      <insight id="I6" name="Developer Activity Profiles" complexity="MEDIUM">
        <research>
          Mockus et al. (2002) "Two Case Studies of Open Source Software Development" (TOSEM);
          Robles et al. (2005) "Evolution and Growth in Large Libre Software Projects" (IWPSE 2005)
        </research>
        <description>
          Build per-developer activity profiles: total commits, files touched,
          average commit size, preferred directories, active time windows,
          first/last commit timestamps. Classify developers as core, peripheral,
          or drive-by contributors.
        </description>
        <data-required>timestamp, author, files[].path — already available</data-required>
        <algorithm>
          Per-author aggregation: commit_count, unique_files_touched, avg_files_per_commit,
          first_commit, last_commit, active_days, preferred_directories (top 3 by commit count).
          Classification: "core" (> X% of total commits AND active recently),
          "peripheral" (regular but low volume), "drive-by" (1-2 commits total).
        </algorithm>
        <output>
          Per-author: commit_count, unique_files, avg_commit_size, first_commit,
          last_commit, active_span_days, classification, preferred_directories.
          Global: core_count, peripheral_count, drive_by_count, active_contributor_count.
        </output>
        <value>
          Identifies who the core maintainers are, who has left,
          and whether the contributor base is growing or shrinking.
        </value>
        <wasm-api>getDeveloperProfiles(), getCoreDevelopers()</wasm-api>
      </insight>

      <insight id="I7" name="Directory-Level Risk Score (Composite)" complexity="HIGH">
        <research>
          Nagappan et al. (2006) "Mining Metrics to Predict Component Failures" (ICSE 2006);
          Multiple: composite scoring from hotspot + coupling + ownership + temporal
        </research>
        <description>
          Combine existing per-file metrics into a single composite risk score
          per directory. Weighted combination of: hotspot intensity, coupling density,
          low bus factor, high ownership concentration, burst activity correlation.
        </description>
        <data-required>All existing insight outputs — already computed</data-required>
        <algorithm>
          For each directory:
            hotspot_score = avg(file_hotspot_scores) / max_hotspot_score (normalized 0-1)
            coupling_score = count(coupling_pairs_involving_dir) / total_files_in_dir
            ownership_risk = 1 - avg(knowledge_entropy) (higher = more concentrated)
            bus_factor_risk = 1 / bus_factor (higher = more fragile)
            temporal_risk = burst_commit_rate_in_dir / overall_burst_rate

          Composite: risk = w1*hotspot + w2*coupling + w3*ownership + w4*bus_factor + w5*temporal
          Default weights: w1=0.3, w2=0.2, w3=0.2, w4=0.2, w5=0.1
          Weights should be configurable.
        </algorithm>
        <output>
          Per-directory: composite_risk_score (0-1), component_scores, risk_level (low/medium/high/critical).
          Global: highest_risk_directories (sorted), risk_distribution histogram.
        </output>
        <value>
          Single-number answer to "where should we invest engineering effort?"
          Directly actionable for sprint planning and resource allocation.
          This is the MOST valuable metric for engineering managers.
        </value>
        <wasm-api>getRiskScores(), getHighRiskDirectories(limit)</wasm-api>
      </insight>

      <insight id="I8" name="Revert / Oscillation Detection" complexity="MEDIUM">
        <research>
          Kim et al. (2006) "Automatic Identification of Bug-Introducing Changes" (ASE 2006);
          Shihab et al. (2012) "An Industrial Study on the Risk of Software Changes" (FSE 2012)
        </research>
        <description>
          Detect files that oscillate between Create and Delete, or that are
          modified in rapid succession by different authors (potential conflict).
          Without commit messages, detect reverts by identifying Delete-then-Create
          patterns on the same path within short time windows.
        </description>
        <data-required>timestamp, author, files[].path, files[].action — already available</data-required>
        <algorithm>
          For each file path, build an action timeline: [(timestamp, action, author)].
          Detect patterns:
            - "oscillation": Create->Delete->Create within X days
            - "rapid handoff": modifications by > 3 different authors within 7 days
            - "conflict signal": same file modified by different authors within 24 hours
          Count occurrences per file, flag high-oscillation files.
        </algorithm>
        <output>
          Per-file: oscillation_count, rapid_handoff_count, conflict_signal_count.
          Global: total_oscillating_files, most_contested_files.
        </output>
        <value>
          Highlights files causing team friction and potential merge conflicts.
          Suggests architectural boundaries that may need clearer ownership.
        </value>
        <wasm-api>getConflictSignals(), getOscillatingFiles(limit)</wasm-api>
      </insight>

      <insight id="I9" name="Commit Rhythm and Cadence Analysis" complexity="LOW">
        <research>
          Eyolfson et al. (2014) "Correlations Between Bugginess and Time" (MSR 2014);
          Sliwerski et al. (2005) "When Do Changes Induce Fixes?" (MSR 2005)
        </research>
        <description>
          Analyze the distribution of inter-commit intervals per author.
          Identify: regular committers (consistent cadence), binge committers
          (long gaps then bursts), declining contributors (increasing gap trend).
        </description>
        <data-required>timestamp, author — already available</data-required>
        <algorithm>
          Per-author: compute inter-commit intervals = [t_{i+1} - t_i].
          Compute: mean, median, stddev, coefficient of variation (CV).
          Classify: "regular" (CV &lt; 0.5), "bursty" (CV > 1.5),
          "declining" (linear regression slope of intervals is positive),
          "ramping" (slope is negative, getting more frequent).
        </algorithm>
        <output>
          Per-author: mean_interval, median_interval, cv, classification,
          trend (accelerating/stable/decelerating).
          Global: team_avg_interval, team_cadence_consistency.
        </output>
        <value>
          Identifies contributors who may be disengaging (increasing intervals)
          or burning out (extremely short intervals during bursts).
        </value>
        <wasm-api>getCommitCadence(), getDecliningContributors()</wasm-api>
      </insight>

      <insight id="I10" name="Architectural Modularity Index" complexity="HIGH">
        <research>
          MacCormack et al. (2006) "Exploring the Structure of Complex Software Designs" (Management Science);
          Baldwin &amp; Clark (2000) Design Rules: The Power of Modularity
        </research>
        <description>
          Use change coupling data to build a file dependency graph and compute
          modularity metrics. Well-modular code should have high intra-directory
          coupling but low inter-directory coupling.
        </description>
        <data-required>Coupling data from existing coupling module — already computed</data-required>
        <algorithm>
          Build adjacency matrix from coupling pairs.
          Partition files into directories (communities).
          Compute Newman's modularity Q: Q = (1/2m) * sum[(A_ij - k_i*k_j/2m) * delta(c_i, c_j)]
          Where: A_ij = coupling weight, k_i = degree of file i, m = total edges,
          delta = 1 if same directory, 0 otherwise.
          Q ranges from -0.5 to 1.0; values > 0.3 indicate good modularity.

          Also compute: propagation cost = average number of directories affected
          when one directory changes. Lower = more modular.
        </algorithm>
        <output>
          Global: modularity_Q, propagation_cost, avg_inter_dir_coupling.
          Per-directory: internal_coupling_density, external_coupling_density,
          most_coupled_external_directory.
          Boundary files: files with highest inter-directory coupling (architectural connectors).
        </output>
        <value>
          Quantifies how well the codebase architecture matches its logical structure.
          High propagation cost = changes ripple across many directories = architectural debt.
          Directly informs refactoring prioritization.
        </value>
        <wasm-api>getModularityIndex(), getPropagationCost(), getBoundaryFiles(limit)</wasm-api>
      </insight>
    </tier>

    <!-- ============================================================== -->
    <!-- TIER 2: REQUIRES PARSER EXTENSION                               -->
    <!-- ============================================================== -->

    <tier id="2" name="Requires VCS Parser Extension" priority="MEDIUM">
      <description>
        These insights require data not currently available in CommitRecord.
        Implementation requires extending rource-vcs to parse additional fields
        from VCS log output. Each parser extension must maintain VCS-agnostic
        design and be tested across Git, SVN, and custom log formats.
      </description>

      <insight id="I11" name="Commit Message Mining (Revert/Fix/Refactor Classification)" complexity="HIGH">
        <research>
          Hindle et al. (2008) "Large-Scale Model of Software Development Activity";
          Mockus &amp; Votta (2000) "Identifying Reasons for Software Changes";
          Kim et al. (2006) — SZZ algorithm for fix-inducing changes
        </research>
        <parser-extension>
          Add `message: Option&lt;String&gt;` field to rource_vcs::Commit.
          Parse from: git log --format, svn log, custom format column.
          Must handle: multi-line messages, non-UTF8, empty messages.
        </parser-extension>
        <algorithm>
          Keyword-based classification (regex patterns):
            - "revert" / "rollback" -> revert_commit
            - "fix" / "bug" / "issue" / "patch" -> bugfix_commit
            - "refactor" / "cleanup" / "rename" -> refactor_commit
            - "feat" / "add" / "implement" -> feature_commit
            - "test" / "spec" -> test_commit
          Compute: revert_rate, fix_rate, refactor_ratio over time.
          Detect fix-inducing patterns: commit followed by "fix" commit touching same files.
        </algorithm>
        <value>
          Enables JIT defect prediction (Hassan 2009), release reliability metrics,
          and test investment tracking. High revert rate = quality process issues.
        </value>
      </insight>

      <insight id="I12" name="Diff Size / Change Volume Metrics" complexity="HIGH">
        <research>
          Nagappan &amp; Ball (2005) "Use of Relative Code Churn Measures to Predict System Defect Density";
          Purushothaman &amp; Perry (2005) "Toward Understanding the Rhetoric of Small Source Code Changes"
        </research>
        <parser-extension>
          Add `lines_added: Option&lt;u32&gt;` and `lines_deleted: Option&lt;u32&gt;` to FileChange.
          Parse from: git log --numstat output.
          Not available for all VCS types (SVN requires separate diff parsing).
        </parser-extension>
        <algorithm>
          Relative code churn = (added + deleted) / total_lines_in_file.
          Cannot compute total_lines (no file content), but can compute:
            - Absolute churn per commit: sum(added + deleted)
            - Churn rate per file: total_churn / commit_count
            - Large change detection: commits where churn > threshold
        </algorithm>
        <value>
          Enables precise defect prediction. Large changes are 2-3x more likely
          to introduce defects than small changes (Purushothaman &amp; Perry 2005).
        </value>
      </insight>

      <insight id="I13" name="Merge Commit and Branch Analysis" complexity="HIGH">
        <research>
          Bird &amp; Zimmermann (2012) "Assessing the Value of Branches" (ESEC/FSE 2012);
          Phillips et al. (2014) "Understanding and Improving Software Build Processes"
        </research>
        <parser-extension>
          Add `parents: Vec&lt;String&gt;` (parent commit hashes) to Commit.
          Merge commits have 2+ parents. Parse from: git log --parents.
          Not available for all VCS types.
        </parser-extension>
        <algorithm>
          Build commit DAG from parent references.
          Detect merge commits (|parents| > 1).
          Compute: merge frequency, avg time-to-merge, merge conflict rate
          (files modified in both branches), integration frequency.
        </algorithm>
        <value>
          Measures CI/CD health and team integration practices.
          Low merge frequency with large merges = higher defect risk.
        </value>
      </insight>

      <insight id="I14" name="DORA-Inspired Delivery Metrics" complexity="MEDIUM">
        <research>
          Forsgren et al. (2018) "Accelerate" — DORA metrics;
          Note: True DORA requires deployment data not available in VCS alone
        </research>
        <parser-extension>
          Add `message: Option&lt;String&gt;` (same as I11) plus tag/release detection.
          Parse from: git tag, git log --decorate.
        </parser-extension>
        <algorithm>
          Proxy metrics (VCS-only approximation):
            - Commit frequency as proxy for deployment frequency
            - Time between tagged releases as proxy for lead time
            - Fix-commit density near releases as proxy for change failure rate
          NOTE: These are approximations, NOT true DORA metrics.
          Document limitations honestly.
        </algorithm>
        <value>
          Provides DORA-like metrics without requiring deployment infrastructure.
          Must clearly label as VCS-derived proxies, not actual DORA measurements.
        </value>
      </insight>

      <insight id="I15" name="Socio-Technical Congruence" complexity="VERY HIGH">
        <research>
          Cataldo et al. (2008) "On the Need for Socio-Technical Congruence" (CSCW 2008);
          Kwan et al. (2011) "Does Socio-Technical Congruence Have an Effect?" (ESEC/FSE 2011)
        </research>
        <parser-extension>
          Requires: commit message (for issue references), parent commits (for DAG),
          ideally issue tracker integration (which is beyond VCS scope).
        </parser-extension>
        <algorithm>
          Build technical dependency graph from coupling analysis.
          Build social collaboration graph from co-file-modification.
          Congruence = overlap between technical dependencies and social connections.
          Low congruence = people who SHOULD be communicating (because their code depends
          on each other) are NOT working together.
        </algorithm>
        <value>
          The holy grail of socio-technical analysis. Research shows congruence
          predicts build failures and defects. Extremely impressive in a portfolio
          but requires significant parser and possibly external data extensions.
        </value>
      </insight>
    </tier>

    <!-- ============================================================== -->
    <!-- TIER 3: RESEARCH CONCEPTS (Feasibility TBD)                     -->
    <!-- ============================================================== -->

    <tier id="3" name="Advanced Research Concepts — Feasibility TBD" priority="LOW">
      <description>
        These represent cutting-edge software engineering research that may be
        partially implementable. Feasibility assessment required before commitment.
        Some may be provably impossible with available data.
      </description>

      <insight id="I16" name="JIT Defect Risk Prediction (Simplified)" complexity="VERY HIGH">
        <research>
          Kamei et al. (2013) "A Large-Scale Empirical Study of JIT Quality Assurance" (TSE);
          Hassan (2009) "Predicting Faults Using the Complexity of Code Changes"
        </research>
        <feasibility>
          Full JIT-SDP requires labeled defect data (which fix commits correspond to
          which bug-introducing commits). Without commit messages and issue tracking,
          can only compute the INPUT FEATURES (entropy, experience, file count, etc.)
          not the actual prediction. Could provide a "risk score" based on known
          risk factors, but cannot validate prediction accuracy.
        </feasibility>
        <honest-assessment>
          Without labeled training data, this produces a heuristic risk score,
          NOT a validated prediction model. Must be clearly documented as such.
          Still valuable as a "code smell" indicator, but claims of "prediction"
          would be intellectually dishonest.
        </honest-assessment>
      </insight>

      <insight id="I17" name="Technical Debt Hotspot with ROI Estimation" complexity="VERY HIGH">
        <research>
          Tornhill (2015) "Your Code as a Crime Scene";
          Fontana et al. (2016) "Technical Debt in Software Projects"
        </research>
        <feasibility>
          Hotspot identification is already implemented. Adding ROI estimation
          requires: cost-of-change estimation (how much effort to modify this file)
          and frequency-of-change projection (how often will it change in future).
          Can approximate cost from historical modification count and author count,
          but cannot directly measure refactoring effort.
        </feasibility>
        <honest-assessment>
          The ROI "estimation" would be based on heuristics, not actual cost data.
          Can provide relative rankings (this file is 3x more expensive to maintain
          than that one) but not absolute dollar/hour values.
        </honest-assessment>
      </insight>
    </tier>
  </future-insights>

  <!-- ================================================================== -->
  <!-- SECTION 5: NEXT SESSION IMPLEMENTATION PLAN                         -->
  <!-- ================================================================== -->

  <next-session-plan>
    <title>Recommended Implementation Order for Next Session</title>
    <description>
      Based on value-to-effort ratio, research backing, and data availability,
      the following implementation order is recommended. Each step builds on
      the previous, maximizing the compounding value of each insight.
    </description>

    <phase number="1" name="Low-Hanging Fruit" estimated-new-tests="20-30">
      <description>Quick wins that extend existing modules with minimal new code.</description>
      <step order="1" insight-ref="I3">
        Implement Codebase Growth Trajectory. Extends existing file tracking
        with a running active-file set. Simple accumulator pattern matching
        existing architecture. Provides immediately compelling time-series data.
      </step>
      <step order="2" insight-ref="I4">
        Implement Work-Type Mix Analysis. Trivial classification from existing
        action ratios. Adds meaningful context to commit history without
        complex algorithms.
      </step>
      <step order="3" insight-ref="I9">
        Implement Commit Cadence Analysis. Per-author interval statistics.
        Simple statistics (mean, median, CV) on timestamp deltas.
      </step>
    </phase>

    <phase number="2" name="High-Value Extensions" estimated-new-tests="30-50">
      <description>Extend existing modules with deeper analysis.</description>
      <step order="4" insight-ref="I1">
        Implement Knowledge Map / Knowledge Silos. Extends existing ownership
        module. Shannon entropy over ownership distribution. Identifies
        high-risk concentrated knowledge areas.
      </step>
      <step order="5" insight-ref="I6">
        Implement Developer Activity Profiles. Per-author aggregation with
        core/peripheral/drive-by classification. Builds on existing author
        tracking in ownership module.
      </step>
      <step order="6" insight-ref="I5">
        Implement File Lifecycle Analysis. Track create->modify->delete patterns.
        Survival analysis (Kaplan-Meier). Identify ephemeral and dead files.
      </step>
    </phase>

    <phase number="3" name="Advanced Algorithms" estimated-new-tests="40-60">
      <description>Complex algorithms showcasing advanced data structures.</description>
      <step order="7" insight-ref="I2">
        Implement Developer Collaboration Network. Graph construction from
        file co-modification. Centrality metrics. Connected components.
        Demonstrates graph algorithm expertise.
      </step>
      <step order="8" insight-ref="I8">
        Implement Revert/Oscillation Detection. Timeline pattern matching
        per file. Sliding window conflict detection.
      </step>
      <step order="9" insight-ref="I10">
        Implement Architectural Modularity Index. Newman's modularity Q on
        coupling graph. Propagation cost analysis. The most algorithmically
        impressive metric — demonstrates spectral graph theory application.
      </step>
    </phase>

    <phase number="4" name="Composite Intelligence" estimated-new-tests="20-30">
      <description>Combine all insights into actionable composite scores.</description>
      <step order="10" insight-ref="I7">
        Implement Directory-Level Risk Score. Weighted combination of all
        previously computed metrics. The capstone metric that engineering
        managers actually want: "where should I invest?"
      </step>
    </phase>
  </next-session-plan>

  <!-- ================================================================== -->
  <!-- SECTION 6: SESSION LEARNINGS AND OPTIMIZATION TIPS                  -->
  <!-- ================================================================== -->

  <session-learnings>
    <title>Lessons Learned from Session 1 (Apply to All Future Sessions)</title>

    <lesson id="SL1" category="clippy">
      <problem>format_push_string lint: cargo clippy rejects `json.push_str(&amp;format!(...))`</problem>
      <solution>Use `use std::fmt::Write;` then `let _ = write!(json, ...);` instead</solution>
      <impact>17 errors in initial WASM API file required full rewrite</impact>
      <prevention>Always use write! macro for String building from the start. Never use format! + push_str.</prevention>
    </lesson>

    <lesson id="SL2" category="clippy">
      <problem>too_many_lines lint: functions exceeding 100 lines are rejected</problem>
      <solution>Split large functions into focused helper functions proactively</solution>
      <impact>Both compute_insights() and format_insights_json() had to be split post-hoc</impact>
      <prevention>Design functions to stay under 80 lines. If a function has more than 3 logical sections, split it during initial implementation, not after.</prevention>
    </lesson>

    <lesson id="SL3" category="clippy">
      <problem>Multiple clippy lints caught after initial implementation: missing Default impls, missing doc backticks, redundant closures, indexed loop variables, implicit_hasher</problem>
      <solution>Fixed each individually — added Default impls, backticks for technical terms in docs, iterator-based loops, etc.</solution>
      <prevention>Run clippy EARLY and OFTEN during implementation, not just at the end. Consider running after each module is complete rather than after all modules.</prevention>
    </lesson>

    <lesson id="SL4" category="architecture">
      <problem>JSON serialization without serde requires significant boilerplate</problem>
      <solution>Hand-built JSON via write! macro works but is verbose (~230 lines of serialization code)</solution>
      <assessment>The no-serde approach avoids a dependency but the serialization code is the largest single section of insights.rs. As more insights are added, consider whether a lightweight JSON builder helper function would reduce repetition while staying serde-free.</assessment>
      <prevention>For future modules, consider creating a tiny JSON builder helper (json_object!, json_array! macros) to reduce boilerplate while avoiding serde dependency.</prevention>
    </lesson>

    <lesson id="SL5" category="architecture">
      <problem>Each WASM API endpoint recomputes the full InsightsReport from scratch</problem>
      <solution>Acceptable for now since computation is &lt;10ms for 10k commits</solution>
      <assessment>When insights grow to 10+ modules, consider caching the InsightsReport in the Rource struct after first computation. Current architecture recomputes on every API call.</assessment>
      <prevention>Add InsightsReport caching when total insight modules exceed 8, or when benchmarks show computation time exceeding 50ms.</prevention>
    </lesson>

    <lesson id="SL6" category="testing">
      <problem>Initial implementation had unused field and unused variable warnings</problem>
      <solution>Fixed by removing unused weighted_sum field and using _ pattern</solution>
      <prevention>Run cargo test with warnings-as-errors BEFORE moving to the next module. Never let warnings accumulate.</prevention>
    </lesson>

    <lesson id="SL7" category="workflow">
      <problem>Context compaction interrupted the clippy fix cycle, requiring re-orientation</problem>
      <solution>The continuation prompt successfully resumed work from the compaction summary</solution>
      <prevention>For large implementations, commit working checkpoints more frequently. Consider committing after each module passes its own clippy/test cycle, then squashing at the end.</prevention>
    </lesson>

    <lesson id="SL8" category="performance">
      <problem>Insights computation must have ZERO impact on the 20us rendering frame budget</problem>
      <solution>All computation happens at query time (WASM API call), not per-frame</solution>
      <critical-rule>NEVER add any insight computation to the per-frame render loop. All insight code runs in response to explicit API calls only. If future insights need to be updated incrementally, they must use a separate update path triggered by explicit API calls, not by the frame loop.</critical-rule>
    </lesson>

    <lesson id="SL9" category="data-constraints">
      <problem>Available commit data is limited: no line counts, no commit messages, no DAG structure</problem>
      <solution>Designed all insights to work within these constraints. Some research metrics (JIT-SDP, DORA) are only partially implementable.</solution>
      <critical-rule>NEVER claim an insight provides capabilities it cannot deliver with available data. If an insight requires line counts or commit messages, clearly document this as a limitation and describe what parser extension would be needed. Intellectual honesty is non-negotiable.</critical-rule>
    </lesson>

    <lesson id="SL10" category="testing">
      <problem>Need comprehensive edge case coverage for timestamp handling</problem>
      <solution>Tests cover: epoch (0), positive, and negative (pre-1970) timestamps using Euclidean division</solution>
      <prevention>For every new temporal computation, include test cases for: timestamp=0, large positive, large negative, same-second commits, and out-of-order timestamps.</prevention>
    </lesson>
  </session-learnings>

  <!-- ================================================================== -->
  <!-- SECTION 7: SESSION STARTUP PROTOCOL                                 -->
  <!-- ================================================================== -->

  <session-startup>
    <title>Mandatory Session Startup Protocol</title>
    <description>
      Execute ALL of these steps BEFORE writing any new code.
      Skipping any step is a violation of the PEER REVIEWED PUBLISHED ACADEMIC standard.
    </description>

    <step order="1" name="Verify Clean State">
      <command>cargo test</command>
      <command>cargo clippy --all-targets --all-features -- -D warnings</command>
      <command>cargo fmt --check</command>
      <expected>Zero failures, zero warnings, zero diffs</expected>
    </step>

    <step order="2" name="Read Current Implementation">
      <command>Read crates/rource-core/src/insights/mod.rs</command>
      <command>Read crates/rource-core/src/insights/hotspot.rs</command>
      <command>Read crates/rource-core/src/insights/coupling.rs</command>
      <command>Read crates/rource-core/src/insights/ownership.rs</command>
      <command>Read crates/rource-core/src/insights/temporal.rs</command>
      <command>Read rource-wasm/src/wasm_api/insights.rs</command>
      <expected>Understand all existing types, algorithms, and test coverage</expected>
    </step>

    <step order="3" name="Verify Existing Tests">
      <command>cargo test -p rource-core --lib insights -- --nocapture</command>
      <command>cargo test -p rource-wasm --lib wasm_api::insights -- --nocapture</command>
      <expected>52 tests pass (44 core + 8 WASM)</expected>
    </step>

    <step order="4" name="Plan Implementation">
      <description>
        Review this prompt's Section 5 (Implementation Plan).
        Select the next phase based on where the previous session left off.
        Create a TodoWrite list with specific tasks BEFORE writing any code.
      </description>
    </step>

    <step order="5" name="Run Clippy After Each Module">
      <description>
        After completing each new insight module:
        1. cargo test -p rource-core --lib insights
        2. cargo clippy -p rource-core --all-targets -- -D warnings
        3. cargo fmt
        Fix ALL issues before starting the next module.
        NEVER let warnings accumulate across modules.
      </description>
    </step>
  </session-startup>

  <!-- ================================================================== -->
  <!-- SECTION 8: ARCHITECTURAL CONSTRAINTS                                -->
  <!-- ================================================================== -->

  <architectural-constraints>
    <title>Architectural Rules for Insights Engine</title>

    <constraint id="AC1" name="VCS Agnostic">
      All insight computation MUST operate on CommitRecord, NOT on rource_vcs::Commit directly.
      The conversion layer in rource-wasm/src/wasm_api/insights.rs is the ONLY place
      that knows about rource_vcs types.
    </constraint>

    <constraint id="AC2" name="Zero Frame Impact">
      Insights computation MUST happen at query time via WASM API calls.
      NEVER add insight code to the per-frame render loop (Scene::update, render_phases).
      The 20us frame budget is sacred.
    </constraint>

    <constraint id="AC3" name="No New Dependencies">
      Prefer implementing algorithms from scratch over adding crate dependencies.
      The project explicitly avoids serde for core. Use write! macro for JSON serialization.
      If a dependency is truly justified, document why in the commit message.
    </constraint>

    <constraint id="AC4" name="Single-Pass Where Possible">
      New accumulators should integrate into the existing single-pass loop in
      compute_insights() rather than requiring additional passes over the commit data.
      Additional passes are acceptable only if the algorithm genuinely requires
      multi-pass processing (document why).
    </constraint>

    <constraint id="AC5" name="Module Structure">
      Each insight gets its own file in crates/rource-core/src/insights/.
      Each module exposes: an Accumulator type (with new(), record_*(), finalize()),
      and result types. The orchestrator in mod.rs drives the accumulation.
      WASM API in rource-wasm/src/wasm_api/insights.rs adds endpoints.
    </constraint>

    <constraint id="AC6" name="Test Coverage">
      Every new insight module MUST include:
      - Unit tests for the accumulator (empty input, single input, multiple inputs)
      - Edge case tests (zero files, single file, bulk commits, Unicode paths)
      - Correctness tests (verify algorithm produces known-correct output for hand-computed examples)
      Minimum: 5 tests per module. Target: 8-12 tests per module.
    </constraint>

    <constraint id="AC7" name="Documentation">
      Every public type and function MUST have doc comments including:
      - Research citation (author, year, venue)
      - Algorithm description
      - Complexity analysis (Big-O)
      - Example output format
    </constraint>

    <constraint id="AC8" name="Function Length">
      No function may exceed 100 lines (clippy::too_many_lines).
      Design functions to stay under 80 lines. Split proactively.
    </constraint>

    <constraint id="AC9" name="JSON Serialization Pattern">
      Use `use std::fmt::Write;` and `let _ = write!(json, ...);` for all JSON construction.
      NEVER use `json.push_str(&amp;format!(...))`. This triggers clippy::format_push_string.
      Consider creating shared JSON builder helpers if serialization boilerplate
      exceeds 50 lines per endpoint.
    </constraint>
  </architectural-constraints>

  <!-- ================================================================== -->
  <!-- SECTION 9: QUALITY GATE CHECKLIST                                   -->
  <!-- ================================================================== -->

  <quality-gates>
    <title>Pre-Commit Quality Gate Checklist</title>
    <description>
      EVERY commit MUST pass ALL of these gates. No exceptions.
    </description>

    <gate id="QG1" name="Tests">cargo test (ALL workspace tests pass, zero failures)</gate>
    <gate id="QG2" name="Clippy">cargo clippy --all-targets --all-features -- -D warnings (zero warnings)</gate>
    <gate id="QG3" name="Format">cargo fmt --check (zero diffs)</gate>
    <gate id="QG4" name="Insight Tests">cargo test -p rource-core --lib insights -- --nocapture (all pass)</gate>
    <gate id="QG5" name="WASM Tests">cargo test -p rource-wasm --lib wasm_api::insights -- --nocapture (all pass)</gate>
    <gate id="QG6" name="No Unused">Zero unused imports, variables, or dead code warnings</gate>
    <gate id="QG7" name="Function Length">No function exceeds 100 lines</gate>
    <gate id="QG8" name="Documentation">All public types and functions have doc comments</gate>
  </quality-gates>

  <!-- ================================================================== -->
  <!-- SECTION 10: FILE INVENTORY                                          -->
  <!-- ================================================================== -->

  <file-inventory>
    <title>Complete File Inventory for Insights Engine</title>

    <file path="crates/rource-core/src/insights/mod.rs" status="EXISTS" tests="9">
      Orchestrator. VCS-agnostic input types. compute_insights() entry point.
      Helper functions: empty_report(), accumulate_commit_data(), finalize_hotspots(),
      finalize_couplings(), finalize_ownership(), compute_summary().
      Constants: BULK_COMMIT_THRESHOLD=50, MIN_COUPLING_SUPPORT=2, DEFAULT_TOP_N=50.
    </file>

    <file path="crates/rource-core/src/insights/hotspot.rs" status="EXISTS" tests="7">
      FileHotspot, HotspotAccumulator. Exponential decay weighting.
    </file>

    <file path="crates/rource-core/src/insights/coupling.rs" status="EXISTS" tests="8">
      CouplingPair, CouplingAccumulator. Sparse co-change matrix.
    </file>

    <file path="crates/rource-core/src/insights/ownership.rs" status="EXISTS" tests="10">
      FileOwnership, ContributorShare, DirectoryBusFactor. Greedy set cover.
    </file>

    <file path="crates/rource-core/src/insights/temporal.rs" status="EXISTS" tests="11">
      TemporalReport, ChangeBurst, TemporalAccumulator. Heatmap + burst detection.
    </file>

    <file path="rource-wasm/src/wasm_api/insights.rs" status="EXISTS" tests="8">
      WASM API layer. 6 endpoints. JSON serialization. Commit conversion.
    </file>

    <file path="crates/rource-core/src/lib.rs" status="MODIFIED">
      Added: pub mod insights;
    </file>

    <file path="rource-wasm/src/wasm_api/mod.rs" status="MODIFIED">
      Added: mod insights;
    </file>

    <!-- Files to create in future sessions -->
    <file path="crates/rource-core/src/insights/growth.rs" status="TO_CREATE">I3: Codebase Growth Trajectory</file>
    <file path="crates/rource-core/src/insights/work_type.rs" status="TO_CREATE">I4: Work-Type Mix Analysis</file>
    <file path="crates/rource-core/src/insights/cadence.rs" status="TO_CREATE">I9: Commit Cadence Analysis</file>
    <file path="crates/rource-core/src/insights/knowledge.rs" status="TO_CREATE">I1: Knowledge Map / Silos</file>
    <file path="crates/rource-core/src/insights/profiles.rs" status="TO_CREATE">I6: Developer Activity Profiles</file>
    <file path="crates/rource-core/src/insights/lifecycle.rs" status="TO_CREATE">I5: File Lifecycle Analysis</file>
    <file path="crates/rource-core/src/insights/network.rs" status="TO_CREATE">I2: Collaboration Network</file>
    <file path="crates/rource-core/src/insights/oscillation.rs" status="TO_CREATE">I8: Revert/Oscillation Detection</file>
    <file path="crates/rource-core/src/insights/modularity.rs" status="TO_CREATE">I10: Architectural Modularity Index</file>
    <file path="crates/rource-core/src/insights/risk.rs" status="TO_CREATE">I7: Composite Risk Score</file>
  </file-inventory>

  <!-- ================================================================== -->
  <!-- SECTION 11: RESEARCH BIBLIOGRAPHY                                   -->
  <!-- ================================================================== -->

  <bibliography>
    <title>Research Citations (Implemented and Planned)</title>
    <description>
      Every insight MUST cite its research basis. This is a PEER REVIEWED
      PUBLISHED ACADEMIC standard project. Claims without citations are invalid.
    </description>

    <!-- Already implemented -->
    <citation id="C1" status="IMPLEMENTED">Nagappan, N. &amp; Ball, T. (2005). "Use of Relative Code Churn Measures to Predict System Defect Density." ICSE 2005.</citation>
    <citation id="C2" status="IMPLEMENTED">D'Ambros, M., Lanza, M., &amp; Robbes, R. (2009). "On the Relationship Between Change Coupling and Software Defects." WCRE 2009.</citation>
    <citation id="C3" status="IMPLEMENTED">Bird, C. et al. (2011). "Don't Touch My Code! Examining the Effects of Ownership on Software Quality." ESEC/FSE 2011.</citation>
    <citation id="C4" status="IMPLEMENTED">Avelino, G. et al. (2016). "A Novel Approach for Estimating Truck Factors." ICPC 2016.</citation>
    <citation id="C5" status="IMPLEMENTED">Eyolfson, J., Tan, L., &amp; Lam, P. (2011). "Do Time of Day and Developer Experience Affect Commit Bugginess?" MSR 2011.</citation>
    <citation id="C6" status="IMPLEMENTED">Hassan, A.E. (2009). "Predicting Faults Using the Complexity of Code Changes." ICSE 2009.</citation>
    <citation id="C7" status="IMPLEMENTED">Nagappan, N. et al. (2008). "The Influence of Organizational Structure on Software Quality." ICSE 2008.</citation>

    <!-- Planned for future -->
    <citation id="C8" status="PLANNED">Rigby, P. &amp; Bird, C. (2013). "Convergent Contemporary Software Peer Review Practices." ESEC/FSE 2013.</citation>
    <citation id="C9" status="PLANNED">Fritz, T. et al. (2014). "Degree-of-Knowledge: Modeling a Developer's Knowledge of Code." TOSEM 2014.</citation>
    <citation id="C10" status="PLANNED">Meneely, A. &amp; Williams, L. (2011). "Socio-Technical Developer Networks." ESEM 2011.</citation>
    <citation id="C11" status="PLANNED">Bird, C. et al. (2009). "Putting It All Together: Using Socio-Technical Networks." ICSE 2009.</citation>
    <citation id="C12" status="PLANNED">Lehman, M.M. (1996). "Laws of Software Evolution Revisited." EWSPT 1996.</citation>
    <citation id="C13" status="PLANNED">Hindle, A. et al. (2008). "Large-Scale Model of Software Development Activity." MSR 2008.</citation>
    <citation id="C14" status="PLANNED">Mockus, A. &amp; Votta, L.G. (2000). "Identifying Reasons for Software Changes Using Historic Databases." ICSM 2000.</citation>
    <citation id="C15" status="PLANNED">Godfrey, M.W. &amp; Tu, Q. (2000). "Evolution in Open Source Software: A Case Study." IWPSE 2000.</citation>
    <citation id="C16" status="PLANNED">Gall, H. et al. (1998). "Detection of Logical Coupling Based on Product Release History." ICSM 1998.</citation>
    <citation id="C17" status="PLANNED">Mockus, A. et al. (2002). "Two Case Studies of Open Source Software Development." TOSEM 2002.</citation>
    <citation id="C18" status="PLANNED">Nagappan, N. et al. (2006). "Mining Metrics to Predict Component Failures." ICSE 2006.</citation>
    <citation id="C19" status="PLANNED">Kim, S. et al. (2006). "Automatic Identification of Bug-Introducing Changes." ASE 2006.</citation>
    <citation id="C20" status="PLANNED">MacCormack, A. et al. (2006). "Exploring the Structure of Complex Software Designs: An Empirical Study of Open Source and Proprietary Code." Management Science 2006.</citation>
    <citation id="C21" status="PLANNED">Cataldo, M. et al. (2008). "On the Need for Socio-Technical Congruence." CSCW 2008.</citation>
    <citation id="C22" status="PLANNED">Kamei, Y. et al. (2013). "A Large-Scale Empirical Study of Just-in-Time Quality Assurance." TSE 2013.</citation>
    <citation id="C23" status="PLANNED">Tornhill, A. (2015). "Your Code as a Crime Scene." Pragmatic Bookshelf.</citation>
    <citation id="C24" status="PLANNED">Forsgren, N. et al. (2018). "Accelerate: The Science of Lean Software and DevOps." IT Revolution Press.</citation>
    <citation id="C25" status="PLANNED">Shihab, E. et al. (2012). "An Industrial Study on the Risk of Software Changes." FSE 2012.</citation>
  </bibliography>

</session-prompt>

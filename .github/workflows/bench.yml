name: Benchmarks

on:
  push:
    branches: [main]
  workflow_dispatch:

# Prevent concurrent benchmark runs — critical for reproducibility
concurrency:
  group: benchmarks-${{ github.ref }}
  cancel-in-progress: true

# Permissions for benchmark storage
permissions:
  contents: write      # Required for gh-pages push
  deployments: write   # Required for deployment status

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  # Explicit list of Criterion benchmark targets.
  #
  # WHY EXPLICIT: We must exclude iai-callgrind benchmarks (iai_scene, iai_vcs)
  # which require valgrind and use a completely different harness/output format.
  # Using --workspace --benches would also run library test harnesses, which
  # reject Criterion CLI flags and cause "Unrecognized option" errors.
  #
  # MAINTENANCE: Update this list when adding/removing [[bench]] targets in
  # crate Cargo.toml files. Each entry must have harness = false and use
  # criterion_main! macro. iai-callgrind benchmarks are tracked separately.
  #
  # Current Criterion benchmarks (13 total across 4 crates):
  #   rource-math:   color_perf
  #   rource-vcs:    vcs_parsing
  #   rource-core:   scene_perf, easing_perf, barnes_hut_theta
  #   rource-render: bloom_perf, blend_perf, visual_perf, render_scale,
  #                  texture_batching, primitive_consolidation, disc_perf, label_perf
  CRITERION_BENCHES: >-
    --bench color_perf
    --bench vcs_parsing
    --bench scene_perf
    --bench easing_perf
    --bench barnes_hut_theta
    --bench bloom_perf
    --bench blend_perf
    --bench visual_perf
    --bench render_scale
    --bench texture_batching
    --bench primitive_consolidation
    --bench disc_perf
    --bench label_perf

jobs:
  # Setup gh-pages branch if it doesn't exist (runs once)
  setup-gh-pages:
    name: Setup gh-pages Branch
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    outputs:
      branch_exists: ${{ steps.check.outputs.exists }}
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Check if gh-pages branch exists
        id: check
        run: |
          if git ls-remote --heads origin gh-pages | grep -q gh-pages; then
            echo "gh-pages branch already exists"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "gh-pages branch does not exist, will create it"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Create gh-pages branch
        if: steps.check.outputs.exists == 'false'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Create orphan branch with initial commit
          git checkout --orphan gh-pages
          git reset --hard

          # Create initial structure
          mkdir -p dev/bench dev/sizes

          cat > README.md << 'EOF'
          # Benchmark Results

          This branch contains benchmark and size metrics for the rource project.

          ## Structure

          - `dev/bench/` - Performance benchmark data
          - `dev/sizes/` - Binary size tracking data

          ## Usage

          View the benchmark dashboard at:
          https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/dev/bench/

          ---
          *Auto-generated by GitHub Actions*
          EOF

          cat > dev/bench/.gitkeep << 'EOF'
          # Placeholder for benchmark data
          EOF

          cat > dev/sizes/.gitkeep << 'EOF'
          # Placeholder for size metrics
          EOF

          git add -A
          git commit -m "Initialize gh-pages branch for benchmarks and metrics"
          git push origin gh-pages

          echo "Created gh-pages branch successfully"

  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [setup-gh-pages]
    # Run even if setup-gh-pages was skipped (for workflow_dispatch)
    if: always() && (needs.setup-gh-pages.result == 'success' || needs.setup-gh-pages.result == 'skipped')
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v6

      - uses: dtolnay/rust-toolchain@stable

      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: "v1-rust"
          shared-key: "bench"

      - name: Run Criterion benchmarks
        run: |
          # Run ONLY Criterion benchmarks by explicitly listing bench targets.
          #
          # This avoids three failure modes that broke prior runs:
          #   1. iai-callgrind benchmarks (iai_scene, iai_vcs) need valgrind
          #   2. Library test harnesses reject unknown CLI flags
          #   3. --noplot was removed in Criterion 0.5+ (our version: 0.8)
          #
          # Criterion 0.8 does NOT generate plots by default.
          # No arguments after -- are needed for standard operation.
          cargo bench --workspace ${{ env.CRITERION_BENCHES }} 2>&1 | tee benchmark-raw.txt

          # Verify benchmarks actually produced timing output
          if ! grep -q "time:" benchmark-raw.txt; then
            echo "::error::No Criterion timing results found in benchmark output."
            echo "Check benchmark-raw.txt artifact for compilation or runtime errors."
            exit 1
          fi

          # Create a clean summary (filter out compilation noise, keep results)
          echo "=== Benchmark Results ===" > benchmark-results.txt
          grep -E '(time:|thrpt:|change:)' benchmark-raw.txt >> benchmark-results.txt 2>/dev/null || true

      - name: Extract Criterion results to JSON
        run: |
          # Convert Criterion output to JSON format for benchmark-action.
          # This parser handles the exact Criterion 0.8 output format.
          python3 << 'PYEOF'
          import json
          import re
          import sys

          results = []
          with open('benchmark-raw.txt', 'r') as f:
              content = f.read()

          # Parse Criterion benchmark output.
          #
          # Criterion 0.8 output format (one result per benchmark):
          #   benchmark_name          time:   [1.2345 ns 1.2456 ns 1.2567 ns]
          #                           thrpt:  [7.8 Melem/s 7.9 Melem/s 8.0 Melem/s]
          #
          # CRITICAL: Use \S+ for unit matching, NOT \w+.
          # Criterion outputs Unicode 'µs' (U+00B5) for microseconds.
          # \w+ only matches [a-zA-Z0-9_] and SILENTLY DROPS all µs results,
          # causing "Extracted 0 benchmark results" when all timings are in µs.
          pattern = r'([\w][\w/]*)\s+time:\s+\[([0-9.]+)\s+(\S+)\s+([0-9.]+)\s+(\S+)\s+([0-9.]+)\s+(\S+)\]'

          for match in re.finditer(pattern, content):
              name = match.group(1)
              lower = float(match.group(2))
              lower_unit = match.group(3)
              estimate = float(match.group(4))
              estimate_unit = match.group(5)
              upper = float(match.group(6))
              upper_unit = match.group(7)

              # Convert to nanoseconds for consistent storage and comparison.
              # Criterion unit strings: ps, ns, µs (Unicode), us (ASCII fallback), ms, s
              multipliers = {
                  'ps': 0.001,
                  'ns': 1,
                  '\u00b5s': 1000,  # µs — Unicode micro sign (U+00B5)
                  'us': 1000,       # ASCII fallback (some terminals)
                  'ms': 1000000,
                  's': 1000000000,
              }

              unit_mult = multipliers.get(estimate_unit)
              if unit_mult is None:
                  print(f"WARNING: Unknown unit '{repr(estimate_unit)}' for '{name}', skipping", file=sys.stderr)
                  continue

              value_ns = estimate * unit_mult
              lower_ns = lower * multipliers.get(lower_unit, unit_mult)
              upper_ns = upper * multipliers.get(upper_unit, unit_mult)

              # Confidence interval half-width for range field
              half_width_ns = (upper_ns - lower_ns) / 2.0

              results.append({
                  'name': name,
                  'unit': 'ns',
                  'value': round(value_ns, 3),
                  'range': f'\u00b1 {half_width_ns:.3f}',
              })

          # Write results (benchmark-action needs valid JSON array)
          with open('benchmark-data.json', 'w') as f:
              json.dump(results, f, indent=2)

          print(f"Extracted {len(results)} benchmark results")

          if len(results) == 0:
              print("ERROR: No benchmark results extracted from Criterion output.", file=sys.stderr)
              print("This means the regex did not match any lines in benchmark-raw.txt.", file=sys.stderr)
              print("Debug: first 50 lines containing 'time':", file=sys.stderr)
              for line in content.split('\n'):
                  if 'time' in line.lower():
                      print(f"  {repr(line)}", file=sys.stderr)
              sys.exit(1)
          else:
              # Print summary for CI log
              for r in sorted(results, key=lambda x: x['name']):
                  ns = r['value']
                  if ns >= 1_000_000:
                      human = f"{ns/1_000_000:.2f} ms"
                  elif ns >= 1_000:
                      human = f"{ns/1_000:.2f} \u00b5s"
                  else:
                      human = f"{ns:.2f} ns"
                  print(f"  {r['name']}: {human} ({r['range']} ns)")
          PYEOF

      - name: Verify benchmark extraction
        id: check_benchmarks
        run: |
          RESULT_COUNT=$(python3 -c "import json; print(len(json.load(open('benchmark-data.json'))))")
          echo "result_count=$RESULT_COUNT" >> $GITHUB_OUTPUT
          echo "Extracted $RESULT_COUNT benchmark results"

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push' && github.ref == 'refs/heads/main' && steps.check_benchmarks.outputs.result_count != '0'
        with:
          name: Rust Benchmarks
          tool: 'customSmallerIsBetter'
          output-file-path: benchmark-data.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** \`${{ github.ref_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ ! -f benchmark-data.json ]; then
            echo "> **Error:** Benchmark data file not found. Benchmarks failed to run." >> $GITHUB_STEP_SUMMARY
            echo "> Check the benchmark-raw.txt artifact for details." >> $GITHUB_STEP_SUMMARY
          else
            RESULT_COUNT=$(python3 -c "import json; print(len(json.load(open('benchmark-data.json'))))" 2>/dev/null || echo "0")

            if [ "$RESULT_COUNT" -eq 0 ]; then
              echo "> **Warning:** No benchmark results extracted." >> $GITHUB_STEP_SUMMARY
              echo "> Check the benchmark-raw.txt artifact for Criterion output format." >> $GITHUB_STEP_SUMMARY
            else
              echo "### Results ($RESULT_COUNT benchmarks)" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY

              # Create a clean table from JSON
              python3 << 'PYEOF'
          import json

          with open('benchmark-data.json') as f:
              data = json.load(f)

          if data:
              with open('/tmp/bench_table.md', 'w') as out:
                  out.write("| Benchmark | Time (ns) | Time (human) | 95% CI |\n")
                  out.write("|-----------|-----------|--------------|--------|\n")
                  for entry in sorted(data, key=lambda x: x['name']):
                      ns = entry['value']
                      if ns >= 1_000_000:
                          human = f"{ns/1_000_000:.2f} ms"
                      elif ns >= 1_000:
                          human = f"{ns/1_000:.2f} \u00b5s"
                      else:
                          human = f"{ns:.2f} ns"
                      ci = entry.get('range', '-')
                      out.write(f"| `{entry['name']}` | {ns:.1f} | {human} | {ci} ns |\n")
          PYEOF
              cat /tmp/bench_table.md >> $GITHUB_STEP_SUMMARY 2>/dev/null || true
            fi
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Reproducibility Notes" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Runner**: GitHub Actions \`ubuntu-latest\` (shared infrastructure)" >> $GITHUB_STEP_SUMMARY
          echo "- **CPU governor**: Uncontrolled (expect 5-15% variance between runs)" >> $GITHUB_STEP_SUMMARY
          echo "- **Framework**: Criterion 0.8, 100+ samples per benchmark, 95% CI" >> $GITHUB_STEP_SUMMARY
          echo "- **Excluded**: iai-callgrind benchmarks (require valgrind, deterministic instruction counts)" >> $GITHUB_STEP_SUMMARY
          echo "- **Alert threshold**: 110% (10% regression triggers alert)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> For picosecond-precision measurements, run locally with CPU governor" >> $GITHUB_STEP_SUMMARY
          echo "> pinned to \`performance\` mode and turbo boost disabled." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Run on-demand via Actions > Benchmarks > Run workflow*" >> $GITHUB_STEP_SUMMARY

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results
          path: |
            benchmark-raw.txt
            benchmark-results.txt
            benchmark-data.json

  # Size tracking — monitor binary and WASM sizes
  size-check:
    name: Size Check
    runs-on: ubuntu-latest
    needs: [setup-gh-pages]
    # Run even if setup-gh-pages was skipped
    if: always() && (needs.setup-gh-pages.result == 'success' || needs.setup-gh-pages.result == 'skipped')
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v6

      - uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: "v1-rust"
          shared-key: "size-check"

      - name: Install wasm-pack
        uses: jetli/wasm-pack-action@v0.4.0
        with:
          version: 'latest'

      - name: Install binaryen
        run: |
          sudo apt-get update
          sudo apt-get install -y binaryen

      - name: Build release binary
        run: cargo build --release

      - name: Build optimized WASM
        run: |
          cd rource-wasm
          wasm-pack build --target web --release
          cd pkg
          # Feature flags MUST come before input file for validation
          wasm-opt \
            --enable-simd \
            --enable-bulk-memory \
            --enable-sign-ext \
            --enable-nontrapping-float-to-int \
            --enable-mutable-globals \
            -Oz \
            -o rource_wasm_bg_opt.wasm rource_wasm_bg.wasm
          mv rource_wasm_bg_opt.wasm rource_wasm_bg.wasm

      - name: Report sizes
        id: sizes
        run: |
          echo "## Binary Sizes" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Artifact | Size | Gzipped |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|------|---------|" >> $GITHUB_STEP_SUMMARY

          # Native binary
          native_size=$(ls -lh target/release/rource | awk '{print $5}')
          native_bytes=$(stat -c%s target/release/rource)
          native_gzip=$(gzip -c target/release/rource | wc -c | numfmt --to=iec-i --suffix=B)
          echo "| Native binary | $native_size | $native_gzip |" >> $GITHUB_STEP_SUMMARY

          # WASM
          wasm_size=$(ls -lh rource-wasm/pkg/rource_wasm_bg.wasm | awk '{print $5}')
          wasm_bytes=$(stat -c%s rource-wasm/pkg/rource_wasm_bg.wasm)
          wasm_gzip_bytes=$(gzip -c rource-wasm/pkg/rource_wasm_bg.wasm | wc -c)
          wasm_gzip=$(echo $wasm_gzip_bytes | numfmt --to=iec-i --suffix=B)
          echo "| WASM bundle | $wasm_size | $wasm_gzip |" >> $GITHUB_STEP_SUMMARY

          # JS bindings
          js_size=$(ls -lh rource-wasm/pkg/rource_wasm.js | awk '{print $5}')
          echo "| JS bindings | $js_size | - |" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Size Targets" >> $GITHUB_STEP_SUMMARY
          echo "- Native binary: < 5MB" >> $GITHUB_STEP_SUMMARY
          echo "- WASM (gzipped): < 300KB" >> $GITHUB_STEP_SUMMARY

          # Export for later steps
          echo "native_bytes=$native_bytes" >> $GITHUB_OUTPUT
          echo "wasm_bytes=$wasm_bytes" >> $GITHUB_OUTPUT
          echo "wasm_gzip_bytes=$wasm_gzip_bytes" >> $GITHUB_OUTPUT

      - name: Check size limits
        run: |
          native_bytes=${{ steps.sizes.outputs.native_bytes }}
          wasm_gzip_bytes=${{ steps.sizes.outputs.wasm_gzip_bytes }}

          # Check native binary size (< 5MB)
          if [ $native_bytes -gt 5242880 ]; then
            echo "::warning::Native binary exceeds 5MB target: $(numfmt --to=iec-i --suffix=B $native_bytes)"
          fi

          # Check WASM gzipped size (< 300KB)
          if [ $wasm_gzip_bytes -gt 307200 ]; then
            echo "::warning::WASM gzipped exceeds 300KB target: $(numfmt --to=iec-i --suffix=B $wasm_gzip_bytes)"
          fi

      - name: Create size metrics JSON
        run: |
          native_bytes=${{ steps.sizes.outputs.native_bytes }}
          wasm_bytes=${{ steps.sizes.outputs.wasm_bytes }}
          wasm_gzip_bytes=${{ steps.sizes.outputs.wasm_gzip_bytes }}

          cat > size-metrics.json << EOF
          [
            {"name": "Native Binary", "unit": "bytes", "value": $native_bytes},
            {"name": "WASM Bundle", "unit": "bytes", "value": $wasm_bytes},
            {"name": "WASM Gzipped", "unit": "bytes", "value": $wasm_gzip_bytes}
          ]
          EOF

      - name: Store size metrics
        uses: benchmark-action/github-action-benchmark@v1
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        with:
          name: Binary Sizes
          tool: 'customSmallerIsBetter'
          output-file-path: size-metrics.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '120%'
          comment-on-alert: true
          fail-on-alert: false
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/sizes

      - name: Upload size metrics
        uses: actions/upload-artifact@v6
        with:
          name: size-metrics
          path: size-metrics.json
